{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOmnkprKQYiHzQt5/3a/8+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amrutha1103/INFO7390/blob/main/Understanding_data_quiz_questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. What does the term \"tokenization\" refer to in NLP?**\n",
        "\n",
        "a) Converting text to binary code\n",
        "\n",
        "b) Breaking text into smaller units (words or subwords)\n",
        "\n",
        "c) Encoding text using special characters\n",
        "\n",
        "d) Generating synonyms for words\n",
        "\n",
        "\n",
        "**Answer - b) Breaking text into smaller units (words or subwords)**\n",
        "\n",
        "\n",
        "Explantiom -\n",
        "\n",
        " Tokenization in NLP is the process of breaking a given text into smaller units, which are typically words or subwords. These smaller units, called tokens, serve as the building blocks for various NLP tasks, such as text analysis, text classification, and machine learning. Tokenization helps in making text data more manageable and allows NLP models to understand and process language effectively. It is a crucial preprocessing step before any text-based analysis, enabling computers to work with human language in a structured manner.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##2.  What is the purpose of a word embedding technique like Word2Vec in NLP?**\n",
        "\n",
        "\n",
        "\n",
        "a) Translating words to other languages\n",
        "\n",
        "b) Measuring word frequencies in a text\n",
        "\n",
        "c) Representing words as dense vectors in a continuous space\n",
        "\n",
        "d) Summarizing long documents\n",
        "\n",
        "\n",
        "**Answer - c) Representing words as dense vectors in a continuous space**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###Explanation -\n",
        "\n",
        "Word embedding techniques like Word2Vec are used in NLP to represent words as dense vectors in a continuous space. These dense vector representations capture semantic relationships between words, making it easier for NLP models to work with words and understand their contextual meanings. Word embeddings enable NLP models to perform various tasks, such as semantic similarity analysis, sentiment analysis, and text classification, by providing a meaningful numerical representation of words. Translation, measuring word frequencies, and summarizing long documents are not the primary purposes of word embeddings like Word2Vec.\n",
        "\n",
        "\n",
        "\n",
        "Suppose we have a corpus of text that includes the following sentences:\n",
        "\n",
        "* \"The cat chased the mouse.\"\n",
        "\n",
        "\n",
        "* \"The dog barked loudly.\"\n",
        "\n",
        "\n",
        "* \"The mouse ran away quickly.\"\n",
        "\n",
        "\n",
        "\n",
        "Without word embeddings, NLP models might treat each word as a unique entity and struggle to understand relationships between words. However, when we use Word2Vec or similar word embedding techniques, words are represented as dense vectors in a continuous space.\n",
        "\n",
        "Here's a simplified example of how Word2Vec might represent some words:\n",
        "\n",
        "\"cat\" -> [0.3, 0.7, -0.5]\n",
        "\n",
        "\"dog\" -> [0.4, 0.8, -0.3]\n",
        "\n",
        "\"mouse\" -> [0.1, 0.9, -0.2]\n",
        "\n",
        "\"chased\" -> [0.6, 0.5, -0.7]\n",
        "\n",
        "\"barked\" -> [0.7, 0.6, -0.5]\n",
        "\n",
        "\"ran\" -> [0.8, 0.4, -0.6]\n",
        "\n",
        "\n",
        "In this vector space, words with similar meanings or contexts are closer together. For instance:\n",
        "\n",
        "The vectors for \"cat\" and \"dog\" are close because both are related to pets.\n",
        "\n",
        "The vectors for \"chased\" and \"ran\" are close because they represent actions involving movement.\n",
        "\n",
        "The vectors for \"cat\" and \"mouse\" are also relatively close because they are often found together in sentences describing predator-prey relationships."
      ],
      "metadata": {
        "id": "ACDwNnkVMf2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Which of the following is an unsupervised technique used in sentiment analysis?\n",
        "\n",
        "A) Support Vector Machines (SVM)\n",
        "\n",
        "B) Bag of Words (BoW)\n",
        "\n",
        "C) Lexicon-based analysis\n",
        "\n",
        "D) Random Forest\n",
        "\n",
        "**Answer - C) Lexicon-based analysis**\n",
        "\n",
        "Explanation -\n",
        "\n",
        "**Lexicon-based analysis** is an unsupervised technique in sentiment analysis because it doesn't rely on labeled training data or machine learning algorithms. Instead, it uses predefined sentiment lexicons or dictionaries that contain lists of words and their associated sentiment scores. These scores typically indicate whether a word is positive, negative, or neutral in sentiment.\n",
        "\n",
        "\n",
        "**Here's how lexicon-based analysis works:**\n",
        "\n",
        "\n",
        "* **Lexicon Creation**  Sentiment lexicons are created by experts or generated automatically from large corpora of text. Each word in the lexicon is assigned a sentiment score based on its typical sentiment connotation.\n",
        "\n",
        "* **Text Analysis**: When analyzing a piece of text, the lexicon-based approach scans through the text and identifies words from the sentiment lexicon. It assigns sentiment scores to these words based on the lexicon's entries.\n",
        "\n",
        "* **Aggregation:** The sentiment scores of individual words in the text are aggregated to calculate an overall sentiment score for the entire text. Different lexicons may use various aggregation methods, such as summing the scores or taking an average.\n",
        "\n",
        "* **Sentiment Classification:** Based on the aggregated sentiment score, the text is classified as positive, negative, or neutral. Some lexicon-based approaches may also provide a continuous sentiment score, indicating the strength of the sentiment.\n",
        "\n",
        "\n",
        "Because lexicon-based analysis doesn't require training on labeled data, it is considered unsupervised. It's a valuable technique for sentiment analysis tasks when labeled training data is scarce or when you want to quickly assess the sentiment of text without building a machine learning model.\n",
        "\n",
        "\n",
        "\n",
        "In contrast, options A, B, and D (Support Vector Machines, Bag of Words, and Random Forest) are supervised techniques that rely on labeled training data and machine learning algorithms to classify sentiment in text. These methods require a dataset where each piece of text is labeled with its corresponding sentiment (positive, negative, or neutral) during the training phase."
      ],
      "metadata": {
        "id": "0Km1UK9oUvw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WWLnr9ZWWPc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.  Which of the following is an example of a lexicon-based approach to sentiment analysis?\n",
        "\n",
        "a) Recurrent Neural Network (RNN)\n",
        "\n",
        "b) Long Short-Term Memory (LSTM)\n",
        "\n",
        "c) VADER (Valence Aware Dictionary and sEntiment Reasoner)\n",
        "\n",
        "d) Naive Bayes Classifier\n",
        "\n",
        "\n",
        "Answer - c) VADER (Valence Aware Dictionary and sEntiment Reasoner)\n",
        "\n",
        "\n",
        "Explanation - VADER is an example of a lexicon-based approach to sentiment analysis. It uses a predefined sentiment lexicon (a dictionary of words with associated sentiment scores) to analyze and classify the sentiment of text based on the words and their sentiment scores in the text. Lexicon-based approaches like VADER do not rely on machine learning algorithms but rather on predefined rules and word sentiment scores."
      ],
      "metadata": {
        "id": "vcknGhTIWogQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.  Which of the following pre-processing steps is NOT typically performed in text data preparation for sentiment analysis?\n",
        "\n",
        "a) Tokenization\n",
        "\n",
        "b) Stemming\n",
        "\n",
        "c) Feature Extraction\n",
        "\n",
        "d) Lemmatization\n",
        "\n",
        "\n",
        "**Answer: c) Feature Extraction**\\\n",
        "\n",
        "\n",
        "**Explanation**\n",
        "\n",
        "In sentiment analysis, the primary focus of pre-processing is to prepare the text data for analysis by cleaning, standardizing, and simplifying it. Tokenization, stemming, and lemmatization are common pre-processing steps to achieve this. Feature extraction typically refers to converting the text data into numerical features, which is not considered a pre-processing step but rather a feature engineering step that follows pre-processing."
      ],
      "metadata": {
        "id": "8EDfijYJXXEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. In a sentiment analysis task, what is the typical range for sentiment polarity scores using a sentiment lexicon?\n",
        "\n",
        "\n",
        "A) -1 to 1\n",
        "\n",
        "B) 0 to 1\n",
        "\n",
        "C) -5 to 5\n",
        "\n",
        "D) 1 to 10\n",
        "\n",
        "**Answer: C) -5 to 5**\n",
        "\n",
        "\n",
        "**explanation:**\n",
        "\n",
        "Sentiment polarity scores obtained from a sentiment lexicon can typically range from -5 (extremely negative) to 5 (extremely positive) to represent the sentiment intensity."
      ],
      "metadata": {
        "id": "Pc8M5qi1Xr58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. What is the purpose of stemming in text preprocessing for sentiment analysis?\n",
        "\n",
        "A) To remove stop words\n",
        "\n",
        "B) To convert text to lowercase\n",
        "\n",
        "C) To reduce words to their base or root form\n",
        "\n",
        "D) To perform sentiment scoring\n",
        "\n",
        "\n",
        "\n",
        "**Answer : C) To reduce words to their base or root form**\n",
        "\n",
        "**Explantion**\n",
        "\n",
        "Stemming is the process of reducing words to their base or root form, which helps in standardizing text data and reducing dimensionality."
      ],
      "metadata": {
        "id": "NEho20YOYL89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. What is the primary purpose of using a WordNet lemmatizer in natural language processing?\n",
        "\n",
        "A) To remove stop words and punctuation marks\n",
        "\n",
        "B) To split text into individual tokens\n",
        "\n",
        "C) To reduce words to their base or root form\n",
        "\n",
        "D) To perform sentiment analysis on text data\n",
        "\n",
        "\n",
        "\n",
        "**Answer: C) To reduce words to their base or root form**\n",
        "\n",
        "\n",
        "###Explanation\n",
        "\n",
        "The primary purpose of using a WordNet lemmatizer in natural language processing is to reduce words to their base or root form, known as lemmas, based on their meanings and context. This process helps in standardizing text data and reducing dimensionality. For example, consider the word \"running.\" When lemmatized using WordNet, it is reduced to its base form \"run,\" which is a more meaningful representation of the word's core meaning:\n",
        "\n",
        "\n",
        "Original word: \"running\"\n",
        "\n",
        "Lemmatized word: \"run\"\n",
        "\n",
        "This transformation makes it easier to analyze and understand the text data, especially in tasks like sentiment analysis, where the core meaning of words is crucial for determining sentiment polarity."
      ],
      "metadata": {
        "id": "0_kvuLz-ZavU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. In NLP, what is the purpose of the BLEU metric?\n",
        "\n",
        "a) To evaluate the fluency of generated text\n",
        "\n",
        "b) To measure the diversity of language models\n",
        "\n",
        "c) To assess the quality of machine translation\n",
        "\n",
        "d) To quantify the perplexity of a language model\n",
        "\n",
        "**Answer: c) To assess the quality of machine translation**\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "BLEU, which stands for Bilingual Evaluation Understudy, is a metric used to assess the quality of machine-generated translations, particularly in the context of machine translation tasks. Its primary purpose is to provide a quantitative measure of how well an automated translation system, such as a machine translation model, performs compared to human-generated reference translations.\n",
        "\n",
        "\n",
        "\n",
        "## 10. What does POS tagging stand for in NLP?\n",
        "\n",
        "a) Part of Speech Tagging\n",
        "\n",
        "b) Positional Object Syntax\n",
        "\n",
        "c) Post-Operative Sentiment\n",
        "\n",
        "d) Passive Object Segmentation\n",
        "\n",
        "\n",
        "**Answer: a) Part of Speech Tagging**\n",
        "\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "\n",
        "Part-of-speech (POS) tagging is a fundamental task in natural language processing (NLP) that involves the process of assigning grammatical categories or part-of-speech labels to each word in a given text. These labels typically include categories such as nouns, verbs, adjectives, adverbs, pronouns, prepositions, conjunctions, and more. Here's a more detailed explanation of the answer:\n",
        "\n",
        "**a) Part of Speech Tagging:**\n",
        "\n",
        "This is the correct answer. POS tagging, also known as grammatical tagging or word-category disambiguation, is the process of annotating each word in a text with a specific part-of-speech label. These labels help computers understand the syntactic and grammatical structure of a sentence, which is crucial for various NLP tasks.\n",
        "\n",
        "**b) Positional Object Syntax:**\n",
        "\n",
        "This is not the correct answer. \"Positional Object Syntax\" is not a standard term in NLP, and it does not relate to the concept of assigning part-of-speech labels to words.\n",
        "\n",
        "**c) Post-Operative Sentiment:**\n",
        "\n",
        "This is not the correct answer. \"Post-Operative Sentiment\" is not a recognized term in NLP. It combines medical terminology (\"post-operative\") with \"sentiment,\" which is unrelated to POS tagging.\n",
        "\n",
        "**d) Passive Object Segmentation: **\n",
        "\n",
        "This is not the correct answer. \"Passive Object Segmentation\" is not a standard term in NLP, and it does not pertain to the process of labeling words with their part-of-speech information.\n",
        "\n",
        "\n",
        "\n",
        "In summary, POS tagging, or \"Part of Speech Tagging,\" is a crucial NLP task that involves assigning grammatical categories to words in a text, helping computers understand the linguistic structure of sentences and enabling various language processing tasks."
      ],
      "metadata": {
        "id": "_PaevwupZxYr"
      }
    }
  ]
}